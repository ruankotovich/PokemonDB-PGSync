# PkmDB - ElasticSearch synchronized with PostgresSQL

## Installation / Deploy

#### 1 - Dry Run

This stage is very important, it is the one that populates the database with ** schemas ** and ** data from the json file **. It is possible to use the `-d` flag at any stage of the docker-compose, but for visualization purposes, it may be better not to use it.

#### 1.1 Test (Optional step?)
This step can be ignored, but I strongly recommend it, for purposes of interfacing with the deployment pattern of test cases that can be performed by a CI / CD. There are some tests focusing on API endpoints. One of the tests required a mock for elasticsearch. You can run the tests with the following command:

``
docker-compose --file docker-compose.test.yaml up
``

#### 1.2 Database Deploy
This architecture implements the microservice principle, so the database, the synchronization service and the server application were separated.

The database service must be started first, since the application service will populate it (it is possible to start the application before, in this case, I used the commonly used `wait-for-it.sh` script to create waiting time for the database).

For this, it is necessary to use the container composer informing the environment:

`` sh
cd infra / services / postgres
docker-compose --env-file ../../default.env up
``

#### 1.3 Migration of JSON data to the database

In this step, I prepared a composer for the dry-run, it is possible to start by executing the following command:

``
docker-compose --file docker-compose.dryrun.yaml up
``

Once executed, the database is active and populated, and the dry-run service has ended

! [] (. README_images / 744ed6b7.png)


### 2 - Deploy

In the deployment stage, we started the database synchronization service with Elasticsearch

#### 2.1 Starting the PGSync service

In this step, the PGSync service starts with Elasticsearch (for search indexes) and Redis (for storing synchronization shards) and makes the Elasticsearch service available to the application

`` sh
cd infra / services / pgsync
docker-compose --env-file ../../default.env up
``

Once synchronization has started, we can consume these 2 services through our API.

#### 2.2 Launch API

To start the API, you can run the command

``
docker-compose --file docker-compose.application.yaml up
``

Right after that, the API waits for Postgres and Elasticsearch to start (for 60 seconds), and then it is available for consumption.

## Data Organization

### Migration of the JSON file

For the migration of the received JSON file, I decided to use the project structure itself to take advantage of the data entry models, so the `scripts / dry-run.ts` file basically reads the given JSON (located in` assets / pkm. json`), go through the file linearly, assembling a list of ** Pokemons ** and ** Types ** with their IDs generated by lib `uuid` (in this case it is necessary to generate them before to be able to insert the relationships of the pokemon with the types in the database), the input of all this data was carried out in a database transaction, to ensure that all relationships go unbroken for storage, or not.

### Database Diagram

I chose to use M: N relations to map the Pok√©mon with the types and with the Teams.

! [] (. README_images / c846974d.png)

The ** Types ** table stores the types belonging to the pokemons, the ** Pokemons ** table stores the pokemons themselves, the ** Teams ** table stores the teams. The ** TypeOfPokemons ** and ** PokemonsOfTeams ** tables store M: N relationships between related entities.

### Elasticsearch

Elasticsearch serves very well in this situation, where an ethical data set must be maintained (in this case, in sync with the original database) and at the same time expose an efficient search interface, in this case, PGSYNC has been configured to mirror the database schema in a partial way, with some attributes of the tables ** Pokemons ** and ** Types **, the configuration file is found in `infra / services / pgsync / schema.json` and has the information of behaviors of the tables described above.

## Usage

In the [documentation] file (./ Documentation.md) there are the routes available through the service.

Below is a short summary of them:

- [GET] ** /rest/pokemons ** - It is possible to pass 4 optional parameters, ** name **, to search for pokemons by name, with an error of up to 2 characters provided by Elasticsearch; ** type **, to search for pokemons by type, also with up to 2 errors in the string; size, to limit the size of the results window; from, to walk through the results window.
- [GET] ** /rest/pokemons/:id ** - Returns a pokemon by ID
- [GET] ** /rest/teams ** - Returns the list of all registered teams
- [GET] ** /rest/teams/:id ** - Returns a team by ID
- [POST] ** /rest/teams ** - Register a new team, following the necessary requirements (from 1 to 6 pokemon, team name and trainer with at least 5 characters)
- [PUT] ** /rest/teams/:id ** - Change a team by ID, it is possible to change both pokemon and team data by this route
- [DELETE] ** /rest/teams/:id ** - Deletes a team by ID